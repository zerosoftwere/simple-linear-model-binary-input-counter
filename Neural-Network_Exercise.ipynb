{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the Gradient Descent Algorithm\n",
    "\n",
    "In this lab, we'll implement the basic functions of the Gradient Descent algorithm to find the number of ones in the input dataset. First, we'll start by importing the math library to help make computation easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading The data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x1  x2  x3  x4  y\n",
       "0   0   0   0   0  0\n",
       "1   0   0   0   1  1\n",
       "2   0   0   1   0  1\n",
       "3   0   0   1   1  2\n",
       "4   0   1   0   0  1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate Data Set from Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.array(data[['x1', 'x2', 'x3', 'x4']])\n",
    "ys = np.array(data['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data Set in to Training Data and Test Data\n",
    "Here we randomly sample the 70% of the data to be used for the training and the other 30% would be used to verify our training accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: [1 0 0 1]\n",
      "Train label:  2\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(xs.shape[0])\n",
    "\n",
    "training_indices = np.random.choice(indices, size=int(np.floor(0.7 * len(indices))), replace=False);\n",
    "test_indices = np.array([x for x in indices if x not in training_indices])\n",
    "\n",
    "training_data = xs[training_indices];\n",
    "training_labels = ys[training_indices];\n",
    "\n",
    "test_data = xs[test_indices];\n",
    "test_labels = ys[test_indices];\n",
    "\n",
    "print('Train data:', training_data[0])\n",
    "print('Train label: ', training_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the basic functions\n",
    "Here is your turn to shine. Implement the following formulas, as explained in the text.\n",
    "- Output (prediction) formula\n",
    "\n",
    "$$\\hat{y} = (w_1 x_1 + w_2 x_2 + w_3 x_3 + w_4 x_4)$$\n",
    "\n",
    "- Error function\n",
    "\n",
    "$$E = (\\hat{y} - y)^2$$\n",
    "\n",
    "- Gradient function\n",
    "\n",
    "$$E^\\prime = 2(\\hat{y} - y)$$\n",
    "\n",
    "- Mean squared Error function\n",
    "\n",
    "$$MSE = \\frac{1}{2m}\\sum{(\\hat{y} - y)^2}$$\n",
    "\n",
    "\n",
    "- Deferential of Mean squared Error function\n",
    "\n",
    "$$MSE^\\prime = \\frac{1}{m}\\sum{(\\hat{y} - y)}$$\n",
    "\n",
    "- The function that updates the weights\n",
    "\n",
    "$$ w_i \\longrightarrow w_i - \\frac{\\alpha}{m}\\sum{(\\hat{y} - y)}x_i$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Output (prediction) formula\n",
    "def output_formula(features, weights):\n",
    "    return np.dot(features, weights)\n",
    "\n",
    "# Error formula\n",
    "def error_formula(y_hat, y):\n",
    "    return (y_hat - y) ** 2\n",
    "\n",
    "def error_formula_prime(y_hat, y):\n",
    "    return 2 * (y_hat - y)\n",
    "\n",
    "def MSE(y_hats, ys):\n",
    "    return np.mean((y_hats - ys) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function\n",
    "This function will help us iterate the gradient descent algorithm through all the data, for a number of epochs.\n",
    "\n",
    "- Prediction fuction \n",
    "\n",
    "$$\\hat{y} = (w_1 x_1 + w_2 x_2 + w_3 x_3 + w_4 x_4)$$\n",
    "\n",
    "- Compute the gradinet \n",
    "\n",
    "$$\\delta = \\frac{1}{m}\\sum{(\\hat{y} - y)}$$\n",
    "\n",
    "- Update the weights\n",
    "\n",
    "$$ w_i \\longrightarrow w_i - \\alpha * \\delta * x_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(44)\n",
    "\n",
    "def train(xs, ys, weights, epochs, learnrate):\n",
    "    \n",
    "    errors = []\n",
    "    n_records, n_features = xs.shape\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for x, y in zip(xs, ys):\n",
    "            # Feed Forward function\n",
    "            y_hat = output_formula(x, weights)\n",
    "            # Compute gradient\n",
    "            d_error = error_formula_prime(y_hat, y)\n",
    "            # Update weights\n",
    "            weights = weights - learnrate * d_error * x\n",
    "        \n",
    "        \n",
    "        # Printing out the loss error on the training set\n",
    "        y_hats = output_formula(xs, weights)\n",
    "        loss = MSE(y_hats, ys)\n",
    "        errors.append(loss)\n",
    "        \n",
    "        if e % (epochs / 10) == 0:\n",
    "            print(\"\\n========== Epoch\", e,\"==========\")\n",
    "            print(\"Train loss: \", loss)\n",
    "    \n",
    "    # Plotting the error\n",
    "    plt.plot(errors)\n",
    "    plt.show()\n",
    "            \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Random Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights [-0.0523222   0.55547164 -0.96340369 -0.18032147]\n"
     ]
    }
   ],
   "source": [
    "# Intialize weights\n",
    "weights = np.random.normal(size=4)\n",
    "print('Weights', weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to train the algorithm!\n",
    "The training algorithim uses a learning rate ($\\alpha$) of 0.1 and a total of 100 iterations (epochs) to train the model over the training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Epoch 0 ==========\n",
      "Train loss:  0.491555877408\n",
      "\n",
      "========== Epoch 4 ==========\n",
      "Train loss:  0.00247291907219\n",
      "\n",
      "========== Epoch 8 ==========\n",
      "Train loss:  1.36849245574e-05\n",
      "\n",
      "========== Epoch 12 ==========\n",
      "Train loss:  8.98905989905e-08\n",
      "\n",
      "========== Epoch 16 ==========\n",
      "Train loss:  8.41567295379e-10\n",
      "\n",
      "========== Epoch 20 ==========\n",
      "Train loss:  1.12395688996e-11\n",
      "\n",
      "========== Epoch 24 ==========\n",
      "Train loss:  1.77305998342e-13\n",
      "\n",
      "========== Epoch 28 ==========\n",
      "Train loss:  2.9080016315e-15\n",
      "\n",
      "========== Epoch 32 ==========\n",
      "Train loss:  4.75957473432e-17\n",
      "\n",
      "========== Epoch 36 ==========\n",
      "Train loss:  7.71674754111e-19\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reintialize weights\n",
    "weights = np.random.normal(size=4)\n",
    "# Perform training\n",
    "weights = train(training_data, training_labels, weights, 40, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [ 1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "print('Weights:', weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100%\n"
     ]
    }
   ],
   "source": [
    "pred = output_formula(test_data, weights)\n",
    "accuracy = np.mean(np.abs(pred - test_labels.astype(np.float32)) < 0.00001)\n",
    "print('Accuracy: %d%%' % (accuracy * 100));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
