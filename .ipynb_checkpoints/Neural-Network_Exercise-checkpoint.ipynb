{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the Gradient Descent Algorithm\n",
    "\n",
    "In this lab, we'll implement the basic functions of the Gradient Descent algorithm to find the number of ones in the input dataset. First, we'll start by importing the math library to help make computation easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading The data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x1  x2  x3  x4  y\n",
       "0   0   0   0   0  0\n",
       "1   0   0   0   1  1\n",
       "2   0   0   1   0  1\n",
       "3   0   0   1   1  2\n",
       "4   0   1   0   0  1"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate Data Set from Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.array(data[['x1', 'x2', 'x3', 'x4']])\n",
    "ys = np.array(data['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data Set in to Training Data and Test Data\n",
    "Here we randomly sample the 70% of the data to be used for the training and the other 30% would be used to verify our training accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: [0 1 1 1]\n",
      "Train label:  3\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(xs.shape[0])\n",
    "\n",
    "training_indices = np.random.choice(indices, size=int(np.floor(0.7 * len(indices))), replace=False);\n",
    "test_indices = np.array([x for x in indices if x not in training_indices])\n",
    "\n",
    "training_data = xs[training_indices];\n",
    "training_labels = ys[training_indices];\n",
    "\n",
    "test_data = xs[test_indices];\n",
    "test_labels = ys[test_indices];\n",
    "\n",
    "print('Train data:', training_data[0])\n",
    "print('Train label: ', training_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the basic functions\n",
    "Here is your turn to shine. Implement the following formulas, as explained in the text.\n",
    "- Output (prediction) formula\n",
    "\n",
    "$$\\hat{y} = (w_1 x_1 + w_2 x_2 + w_3 x_3 + w_4 x_4)$$\n",
    "\n",
    "- Error function\n",
    "\n",
    "$$E = (\\hat{y} - y)^2$$\n",
    "\n",
    "- Gradient function\n",
    "\n",
    "$$E^\\prime = 2(\\hat{y} - y)$$\n",
    "\n",
    "- Mean squared Error function\n",
    "\n",
    "$$MSE = \\frac{1}{2m}\\sum{(\\hat{y} - y)^2}$$\n",
    "\n",
    "\n",
    "- Deferential of Mean squared Error function\n",
    "\n",
    "$$MSE^\\prime = \\frac{1}{m}\\sum{(\\hat{y} - y)}$$\n",
    "\n",
    "- The function that updates the weights\n",
    "\n",
    "$$ w_i \\longrightarrow w_i - \\frac{\\alpha}{m}\\sum{(\\hat{y} - y)}x_i$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Output (prediction) formula\n",
    "def output_formula(features, weights):\n",
    "    return np.dot(features, weights)\n",
    "\n",
    "# Error formula\n",
    "def error_formula(y_hat, y):\n",
    "    return (y_hat - y) ** 2\n",
    "\n",
    "def error_formula_prime(y_hat, y):\n",
    "    return 2 * (y_hat - y)\n",
    "\n",
    "def MSE(y_hats, ys):\n",
    "    return np.mean((y_hats - ys) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function\n",
    "This function will help us iterate the gradient descent algorithm through all the data, for a number of epochs.\n",
    "\n",
    "- Prediction fuction \n",
    "\n",
    "$$\\hat{y} = (w_1 x_1 + w_2 x_2 + w_3 x_3 + w_4 x_4)$$\n",
    "\n",
    "- Compute the gradinet \n",
    "\n",
    "$$\\delta = \\frac{1}{m}\\sum{(\\hat{y} - y)}$$\n",
    "\n",
    "- Update the weights\n",
    "\n",
    "$$ w_i \\longrightarrow w_i - \\alpha * \\delta * x_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(44)\n",
    "\n",
    "def train(xs, ys, weights, epochs, learnrate):\n",
    "    \n",
    "    errors = []\n",
    "    n_records, n_features = xs.shape\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for x, y in zip(xs, ys):\n",
    "            # Feed Forward function\n",
    "            y_hat = output_formula(x, weights)\n",
    "            # Compute gradient\n",
    "            d_error = error_formula_prime(y_hat, y)\n",
    "            # Update weights\n",
    "            weights = weights - learnrate * d_error * x\n",
    "        \n",
    "        \n",
    "        # Printing out the loss error on the training set\n",
    "        y_hats = output_formula(xs, weights)\n",
    "        loss = np.mean(error_formula(y_hats, ys))\n",
    "        errors.append(loss)\n",
    "        \n",
    "        if e % (epochs / 10) == 0:\n",
    "            print(\"\\n========== Epoch\", e,\"==========\")\n",
    "            print(\"Train loss: \", loss)\n",
    "    \n",
    "    # Plotting the error\n",
    "    plt.plot(errors)\n",
    "    plt.show()\n",
    "            \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights [-0.0523222   0.55547164 -0.96340369 -0.18032147]\n"
     ]
    }
   ],
   "source": [
    "# Intialize weights\n",
    "weights = np.random.normal(size=4)\n",
    "print('Weights', weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to train the algorithm!\n",
    "The training algorithim uses a learning rate ($\\alpha$) of 0.1 and a total of 100 iterations (epochs) to train the model over the training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Epoch 0 ==========\n",
      "Train loss:  0.960380449247\n",
      "\n",
      "========== Epoch 10 ==========\n",
      "Train loss:  0.00450397475001\n",
      "\n",
      "========== Epoch 20 ==========\n",
      "Train loss:  2.53684769047e-05\n",
      "\n",
      "========== Epoch 30 ==========\n",
      "Train loss:  1.49037447752e-07\n",
      "\n",
      "========== Epoch 40 ==========\n",
      "Train loss:  8.86492686532e-10\n",
      "\n",
      "========== Epoch 50 ==========\n",
      "Train loss:  5.30689725802e-12\n",
      "\n",
      "========== Epoch 60 ==========\n",
      "Train loss:  3.19251231909e-14\n",
      "\n",
      "========== Epoch 70 ==========\n",
      "Train loss:  1.9288764981e-16\n",
      "\n",
      "========== Epoch 80 ==========\n",
      "Train loss:  1.17004476586e-18\n",
      "\n",
      "========== Epoch 90 ==========\n",
      "Train loss:  7.12352335135e-21\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkkro/wOtLJpn1XxkaQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Perform training\n",
    "weights = train(training_data, training_labels, weights, 100, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = output_formula(test_data, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100%\n"
     ]
    }
   ],
   "source": [
    "accuracy = np.mean(np.abs(pred - test_label.astype(np.float32)) < 0.000001)\n",
    "print('Accuracy: %d%%' % (accuracy * 100));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [-0.0523222   0.55547164 -0.96340369 -0.18032147]\n"
     ]
    }
   ],
   "source": [
    "print('Weights:', weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
